{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12646677,"sourceType":"datasetVersion","datasetId":7992257}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Enable mixed precision\nif tf.config.list_physical_devices('GPU'):\n    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n    print(\"Mixed precision enabled\")\nelse:\n    print(\"Using standard float32\")\n\n# Configuration\nIMG_SIZE = 512\nINPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 2)\nBATCH_SIZE = 4\nAUTOTUNE = tf.data.AUTOTUNE\nEPOCHS_STAGE1 = 40  # Reduced for faster convergence\nEPOCHS_STAGE2 = 60  # Reduced\nTOTAL_EPOCHS = EPOCHS_STAGE1 + EPOCHS_STAGE2\nLR_STAGE1 = 1e-4\nLR_STAGE2 = 1e-5\n\n# ====================================================================\n# ENHANCED DATA PROCESSING WITH AUGMENTATION\n# ====================================================================\n\ndef preprocess_image_for_tfrecord(img_path, mask_path):\n    try:\n        # Read image\n        image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        if image is None:\n            raise ValueError(f\"Failed to read image: {img_path}\")\n        \n        # Handle image channels\n        if image.ndim == 2:\n            image = np.stack([image, image], axis=-1)\n        elif image.shape[-1] > 2:\n            image = image[:, :, :2]  # Use first 2 channels\n        elif image.shape[-1] == 1:\n            image = np.concatenate([image, image], axis=-1)\n\n        # Convert to float32 and normalize (safely)\n        image = image.astype(np.float32)\n        max_val = np.max(image)\n        if max_val > 0:\n            image /= max_val\n\n        # Read mask - FIXED THRESHOLDING\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        if mask is None:\n            raise ValueError(f\"Failed to read mask: {mask_path}\")\n        \n        # CRITICAL FIX: Proper threshold for 8-bit masks (0-255)\n        _, mask = cv2.threshold(mask, 128, 1, cv2.THRESH_BINARY)\n\n        # Resize\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n        mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n\n        return image, np.expand_dims(mask, axis=-1)\n\n    except Exception as e:\n        print(f\"Error processing {img_path}: {str(e)}\")\n        raise\n\ndef create_tfrecord_dataset(image_dir, mask_dir, output_path):\n    \"\"\"Create TFRecords with enhanced numeric filename matching\"\"\"\n    example_count = 0\n    \n    # Check if TFRecord exists\n    if tf.io.gfile.exists(output_path):\n        dataset = tf.data.TFRecordDataset(output_path)\n        example_count = sum(1 for _ in dataset)\n        if example_count > 0:\n            print(f\"TFRecord found with {example_count} examples: {output_path}\")\n            return example_count\n    \n    print(f\"Creating new TFRecord: {output_path}\")\n    \n    # Get all files with numeric sorting\n    image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.tif*\")), \n                        key=lambda x: int(''.join(filter(str.isdigit, os.path.basename(x))) or '0'))\n    mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.tif*\")), \n                       key=lambda x: int(''.join(filter(str.isdigit, os.path.basename(x))) or '0'))\n    \n    # Extract numeric IDs\n    def extract_id(path):\n        filename = os.path.basename(path)\n        digits = ''.join(filter(str.isdigit, filename))\n        return int(digits) if digits else -1\n    \n    image_ids = {extract_id(p): p for p in image_paths}\n    mask_ids = {extract_id(p): p for p in mask_paths}\n    \n    # Find common numeric IDs\n    common_ids = set(image_ids.keys()) & set(mask_ids.keys())\n    \n    if not common_ids:\n        raise ValueError(\"No matching image-mask pairs found.\")\n    \n    print(f\"Found {len(common_ids)} valid image-mask pairs\")\n    \n    # Sort numeric IDs\n    sorted_ids = sorted(common_ids)\n    \n    with tf.io.TFRecordWriter(output_path) as writer:\n        for id_val in sorted_ids:\n            img_path = image_ids[id_val]\n            mask_path = mask_ids[id_val]\n            \n            try:\n                image, mask = preprocess_image_for_tfrecord(img_path, mask_path)\n                \n                feature = {\n                    'image': tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(image).numpy()])\n                    ),\n                    'mask': tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(mask).numpy()])\n                    )\n                }\n                example = tf.train.Example(features=tf.train.Features(feature=feature))\n                writer.write(example.SerializeToString())\n                example_count += 1\n            except Exception as e:\n                print(f\"Skipping ID {id_val} ({img_path}) due to error: {str(e)}\")\n    \n    print(f\"Created TFRecord with {example_count} examples\")\n    return example_count\n\ndef parse_tfrecord(example, augment=False):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.io.parse_tensor(example['image'], out_type=tf.float32)\n    \n    # Convert mask to float32\n    mask = tf.io.parse_tensor(example['mask'], out_type=tf.uint8)\n    mask = tf.cast(mask, tf.float32)\n    \n    image.set_shape(INPUT_SHAPE)\n    mask.set_shape((IMG_SIZE, IMG_SIZE, 1))\n    \n    # Apply augmentations only during training\n    if augment:\n        # Random rotations (0, 90, 180, 270 degrees)\n        k = tf.random.uniform([], maxval=4, dtype=tf.int32)\n        image = tf.image.rot90(image, k)\n        mask = tf.image.rot90(mask, k)\n        \n        # Random flips\n        if tf.random.uniform([]) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n        if tf.random.uniform([]) > 0.5:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n        \n        # Random brightness/contrast (mild)\n        image = tf.image.random_brightness(image, max_delta=0.1)\n        image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n        \n        # Add Gaussian noise (SAR-specific)\n        noise = tf.random.normal(tf.shape(image), mean=0.0, stddev=0.03)\n        image = tf.add(image, noise)\n    \n    return image, mask\n\n# ====================================================================\n# ENHANCED MODEL ARCHITECTURE WITH IMPROVED ATTENTION\n# ====================================================================\n\ndef build_attention_resunet():\n    inputs = tf.keras.Input(shape=INPUT_SHAPE, name=\"input_layer\")\n    \n    # Channel adapter\n    x = tf.keras.layers.Conv2D(\n        3, (1, 1), \n        padding=\"same\", \n        name=\"channel_adapter\",\n        kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n    )(inputs)\n    \n    # Create ResNet backbone\n    resnet_base = tf.keras.applications.ResNet50V2(\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=x,\n        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n    )\n    \n    # Freeze initial layers\n    for layer in resnet_base.layers[:150]:\n        layer.trainable = False\n    \n    # Get feature maps\n    s1 = resnet_base.get_layer(\"conv1_conv\").output\n    s2 = resnet_base.get_layer(\"conv2_block3_1_relu\").output\n    s3 = resnet_base.get_layer(\"conv3_block4_1_relu\").output\n    s4 = resnet_base.get_layer(\"conv4_block6_1_relu\").output\n    bridge = resnet_base.get_layer(\"conv5_block3_1_relu\").output\n\n    # Improved attention gate with residual connection\n    def attention_gate(g, x, inter_channel):\n        g1 = tf.keras.layers.Conv2D(inter_channel, (1, 1), padding='same')(g)\n        g1 = tf.keras.layers.BatchNormalization()(g1)\n        g1 = tf.keras.layers.Activation('relu')(g1)\n        \n        x1 = tf.keras.layers.Conv2D(inter_channel, (1, 1), padding='same')(x)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        \n        psi = tf.keras.layers.Add()([g1, x1])\n        psi = tf.keras.layers.Activation('relu')(psi)\n        psi = tf.keras.layers.Conv2D(1, (1, 1), padding='same', activation='sigmoid')(psi)\n        \n        # Residual connection\n        attented = tf.keras.layers.Multiply()([x, psi])\n        return tf.keras.layers.Add()([x, attented])  # Residual attention\n\n    # Enhanced decoder block with dropout\n    def decoder_block(input_tensor, skip_connection, filters):\n        u = tf.keras.layers.Conv2DTranspose(\n            filters, (2, 2), strides=2, padding='same')(input_tensor)\n        \n        if skip_connection is not None:\n            # Apply attention\n            attn = attention_gate(u, skip_connection, filters)\n            u = tf.keras.layers.Concatenate()([u, attn])\n        \n        # Add spatial dropout for regularization\n        u = tf.keras.layers.SpatialDropout2D(0.2)(u)\n        \n        # Residual block\n        conv1 = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(u)\n        conv1 = tf.keras.layers.BatchNormalization()(conv1)\n        conv1 = tf.keras.layers.Activation('relu')(conv1)\n        \n        conv2 = tf.keras.layers.Conv2D(filters, (3, 3), padding='same')(conv1)\n        conv2 = tf.keras.layers.BatchNormalization()(conv2)\n        \n        # Skip connection\n        if u.shape[-1] == filters:\n            shortcut = u\n        else:\n            shortcut = tf.keras.layers.Conv2D(filters, (1, 1), padding='same')(u)\n        \n        res = tf.keras.layers.Add()([conv2, shortcut])\n        res = tf.keras.layers.Activation('relu')(res)\n        return res\n\n    # Bridge processing with dilation\n    b = tf.keras.layers.Conv2D(1024, (3, 3), padding='same', dilation_rate=2)(bridge)\n    b = tf.keras.layers.BatchNormalization()(b)\n    b = tf.keras.layers.Activation('relu')(b)\n    \n    # Decoder path\n    d1 = decoder_block(b, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n    \n    # Final upsampling\n    u_final = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=2, padding='same')(d4)\n    u_final = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(u_final)\n    \n    # Output layer\n    outputs = tf.keras.layers.Conv2D(\n        1, (1, 1), activation='sigmoid', dtype=tf.float32, name=\"output\"\n    )(u_final)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"EnhancedAttentionResUNet\")\n\n# ====================================================================\n# ENHANCED LOSS & METRICS\n# ====================================================================\n\ndef focal_loss(gamma=3.0, alpha=0.8):  # More focus on hard examples\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n        ce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n        fl = alpha * tf.pow(1 - y_pred, gamma) * ce\n        return tf.reduce_mean(fl)\n    return loss\n\ndef dice_loss(y_true, y_pred):\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n    return 1 - (numerator + 1e-7) / (denominator + 1e-7)\n\ndef hybrid_loss(y_true, y_pred):\n    focal = focal_loss(gamma=3.0, alpha=0.8)(y_true, y_pred)\n    dice = dice_loss(y_true, y_pred)\n    return focal + dice  # Combine both losses\n\ndef iou(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    return (intersection + 1e-7) / (union + 1e-7)\n\n# ====================================================================\n# OPTIMIZED TRAINING PIPELINE\n# ====================================================================\n\n# Paths (update these with your actual paths)\nTRAIN_IMAGES = r\"C:\\Users\\Admin\\Documents\\train\\images\"\nTRAIN_MASKS = r\"C:\\Users\\Admin\\Documents\\train\\mask\"\nVAL_IMAGES = r\"C:\\Users\\Admin\\Documents\\val\\images\"\nVAL_MASKS = r\"C:\\Users\\Admin\\Documents\\val\\mask\"\nTF_TRAIN_PATH = \"/kaggle/input/sar-tfrecords/tfrecords/train.tfrecord\"\nTF_VAL_PATH = \"/kaggle/input/sar-tfrecords/tfrecords/val.tfrecord\"\nOUTPUT_DIR = \"/kaggle/working/\"\n\n# Create output directory\nexperiment_name = f\"oilspill_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\noutput_path = os.path.join(OUTPUT_DIR, experiment_name)\nos.makedirs(output_path, exist_ok=True)\n\n# Model paths\nCHECKPOINT_PATH = os.path.join(output_path, \"checkpoint.weights.h5\")\nFINAL_MODEL_PATH = os.path.join(output_path, \"oil_spill_model.keras\")\nLOG_FILE = os.path.join(output_path, \"training_log.csv\")\n\n# Create TFRecords\nprint(\"\\nCreating training TFRecord...\")\ntrain_count = create_tfrecord_dataset(TRAIN_IMAGES, TRAIN_MASKS, TF_TRAIN_PATH)\nprint(\"\\nCreating validation TFRecord...\")\nval_count = create_tfrecord_dataset(VAL_IMAGES, VAL_MASKS, TF_VAL_PATH)\n\n# Calculate class weights (critical for imbalance)\nprint(\"\\nCalculating class weights...\")\ndef calculate_class_weights():\n    total_pixels = 0\n    oil_pixels = 0\n    \n    dataset = tf.data.TFRecordDataset(TF_TRAIN_PATH)\n    dataset = dataset.map(lambda x: parse_tfrecord(x, augment=False))\n    dataset = dataset.take(100).batch(10)  # Sample 100 batches\n    \n    for images, masks in dataset:\n        oil_pixels += tf.reduce_sum(masks)\n        total_pixels += tf.size(masks)\n    \n    oil_weight = (total_pixels - oil_pixels) / total_pixels\n    background_weight = oil_pixels / total_pixels\n    \n    print(f\"Weights - Oil: {oil_weight:.4f}, Background: {background_weight:.4f}\")\n    return {0: background_weight, 1: oil_weight}\n\nclass_weights = calculate_class_weights()\n\n# Prepare datasets\nsteps_per_epoch = train_count // BATCH_SIZE\nvalidation_steps = max(1, val_count // BATCH_SIZE)\n\n# Training dataset with augmentation\ntrain_ds = (\n    tf.data.TFRecordDataset(TF_TRAIN_PATH)\n    .repeat()\n    .map(lambda x: parse_tfrecord(x, augment=True), num_parallel_calls=AUTOTUNE)\n    .shuffle(100)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)\n\n# Validation dataset without augmentation\nval_ds = (\n    tf.data.TFRecordDataset(TF_VAL_PATH)\n    .repeat()\n    .map(lambda x: parse_tfrecord(x, augment=False), num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)\n\n# Build model\nprint(\"\\nBuilding model...\")\nmodel = build_attention_resunet()\nmodel.summary()\n\n# Diagnostic check\nprint(\"\\nPerforming diagnostic check...\")\nfor images, masks in train_ds.take(1):\n    print(\"Image range:\", tf.reduce_min(images).numpy(), tf.reduce_max(images).numpy())\n    print(\"Mask values:\", tf.unique(tf.reshape(masks, [-1]))[0].numpy())\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121)\n    plt.title(\"SAR Image (VH)\")\n    plt.imshow(images[0, :, :, 1], cmap='gray')\n    plt.subplot(122)\n    plt.title(\"Mask\")\n    plt.imshow(masks[0, :, :, 0], cmap='jet')\n    plt.savefig(os.path.join(output_path, \"sample_data.png\"))\n    plt.close()\n\n# Callbacks\nclass MemorySavingCSVLogger(tf.keras.callbacks.CSVLogger):\n    def on_epoch_end(self, epoch, logs=None):\n        super().on_epoch_end(epoch, logs)\n        tf.keras.backend.clear_session()\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    CHECKPOINT_PATH,\n    save_weights_only=True,\n    save_best_only=True,\n    monitor='val_iou',\n    mode='max',\n    verbose=1\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_iou',\n    mode='max',\n    factor=0.5,\n    patience=8,  # Reduce LR after 8 epochs without improvement\n    min_lr=1e-6,\n    verbose=1\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    patience=30,  # Reduced patience\n    restore_best_weights=True,\n    monitor='val_iou',\n    mode='max'\n)\n\n# Compile model with standard Adam optimizer\nprint(\"\\nCompiling model...\")\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(LR_STAGE1),\n    loss=hybrid_loss,  # Using combined focal + dice loss\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        iou\n    ]\n)\n\n# Train model\nprint(\"\\n\" + \"=\"*50)\nprint(f\"STARTING TRAINING ({TOTAL_EPOCHS} EPOCHS)\")\nprint(\"=\"*50)\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=TOTAL_EPOCHS,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    class_weight=class_weights,  # Critical for imbalance\n    callbacks=[\n        MemorySavingCSVLogger(LOG_FILE), \n        checkpoint, \n        early_stop,\n        reduce_lr\n    ]\n)\n\n# Load best weights and save full model\nprint(\"\\nLoading best weights and saving full model...\")\nmodel.load_weights(CHECKPOINT_PATH)\nmodel.save(FINAL_MODEL_PATH)\nprint(f\"Full model saved to {FINAL_MODEL_PATH}\")\n\n# Clean up checkpoint file\nos.remove(CHECKPOINT_PATH)\nprint(f\"Removed temporary checkpoint: {CHECKPOINT_PATH}\")\n\n# Plot training history\ndef plot_history(history, output_dir):\n    plt.figure(figsize=(15, 10))\n    \n    # Loss\n    plt.subplot(2, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.title('Loss Evolution')\n    plt.legend()\n    \n    # IoU\n    plt.subplot(2, 2, 2)\n    plt.plot(history.history['iou'], label='Train IoU')\n    plt.plot(history.history['val_iou'], label='Val IoU')\n    plt.title('IoU Evolution')\n    plt.legend()\n    \n    # Precision-Recall\n    plt.subplot(2, 2, 3)\n    plt.plot(history.history['precision'], label='Train Precision')\n    plt.plot(history.history['val_precision'], label='Val Precision')\n    plt.plot(history.history['recall'], label='Train Recall')\n    plt.plot(history.history['val_recall'], label='Val Recall')\n    plt.title('Precision & Recall')\n    plt.legend()\n    \n    # Learning Rate\n    if 'lr' in history.history:\n        plt.subplot(2, 2, 4)\n        plt.plot(history.history['lr'], label='Learning Rate')\n        plt.title('Learning Rate Schedule')\n        plt.yscale('log')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n    plt.close()\n\nplot_history(history, output_path)\nprint(\"Training history plot saved\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T20:16:42.275094Z","iopub.execute_input":"2025-08-02T20:16:42.275606Z","iopub.status.idle":"2025-08-02T20:17:39.199117Z","shell.execute_reply.started":"2025-08-02T20:16:42.275582Z","shell.execute_reply":"2025-08-02T20:17:39.198172Z"}},"outputs":[{"name":"stdout","text":"Mixed precision enabled\n\nCreating training TFRecord...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754165803.084885      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"TFRecord found with 2066 examples: /kaggle/input/sar-tfrecords/tfrecords/train.tfrecord\n\nCreating validation TFRecord...\nTFRecord found with 504 examples: /kaggle/input/sar-tfrecords/tfrecords/val.tfrecord\n\nCalculating class weights...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3806955775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbackground_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moil_weight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_class_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;31m# Prepare datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3806955775.py\u001b[0m in \u001b[0;36mcalculate_class_weights\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mtotal_pixels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0moil_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_pixels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moil_pixels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0mbackground_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moil_pixels\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Sub as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Sub] name: "],"ename":"InvalidArgumentError","evalue":"cannot compute Sub as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Sub] name: ","output_type":"error"}],"execution_count":4}]}